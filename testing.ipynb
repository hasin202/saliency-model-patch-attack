{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "psychological-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms, utils, models\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from utils.data_process import preprocess_img, postprocess_img\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "flag = 0 # 0 for TranSalNet_Dense, 1 for TranSalNet_Res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-fetish",
   "metadata": {},
   "source": [
    "↑↑↑  Set **flag=1** to load *TranSalNet_Dense*,set **flag=0** to load *TranSalNet_Res*. <br>\n",
    "<br>\n",
    "↓↓↓  Load the model and pre-trained parameters.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conservative-salmon",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pretrained_models/densenet161-8d451a50.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hasinmahmood/Documents/Year 3/Dis/saliency_patch_attack/testing.ipynb Cell 3\u001b[0m in \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hasinmahmood/Documents/Year%203/Dis/saliency_patch_attack/testing.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hasinmahmood/Documents/Year%203/Dis/saliency_patch_attack/testing.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mTranSalNet_Dense\u001b[39;00m \u001b[39mimport\u001b[39;00m TranSalNet\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hasinmahmood/Documents/Year%203/Dis/saliency_patch_attack/testing.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model \u001b[39m=\u001b[39m TranSalNet()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hasinmahmood/Documents/Year%203/Dis/saliency_patch_attack/testing.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpretrained_models\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mTranSalNet_Dense.pth\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasinmahmood/Documents/Year%203/Dis/saliency_patch_attack/testing.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device) \n",
      "File \u001b[0;32m~/Documents/Year 3/Dis/saliency_patch_attack/TranSalNet_Dense.py:49\u001b[0m, in \u001b[0;36mTranSalNet.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     48\u001b[0m     \u001b[39msuper\u001b[39m(TranSalNet, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m _Encoder()\n\u001b[1;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m _Decoder()\n",
      "File \u001b[0;32m~/Documents/Year 3/Dis/saliency_patch_attack/TranSalNet_Dense.py:61\u001b[0m, in \u001b[0;36m_Encoder.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     60\u001b[0m     \u001b[39msuper\u001b[39m(_Encoder, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> 61\u001b[0m     base_model \u001b[39m=\u001b[39m densenet\u001b[39m.\u001b[39;49mdensenet161(pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     62\u001b[0m     base_layers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(base_model\u001b[39m.\u001b[39mchildren())[\u001b[39m0\u001b[39m][:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList(base_layers)\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Documents/Year 3/Dis/saliency_patch_attack/utils/densenet.py:256\u001b[0m, in \u001b[0;36mdensenet161\u001b[0;34m(pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdensenet161\u001b[39m(pretrained\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, progress\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    247\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Densenet-161 model from\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[39m    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[39mreturn\u001b[39;00m _densenet(\u001b[39m'\u001b[39;49m\u001b[39mdensenet161\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m48\u001b[39;49m, (\u001b[39m6\u001b[39;49m, \u001b[39m12\u001b[39;49m, \u001b[39m36\u001b[39;49m, \u001b[39m24\u001b[39;49m), \u001b[39m96\u001b[39;49m, pretrained, progress,\n\u001b[1;32m    257\u001b[0m                      \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Year 3/Dis/saliency_patch_attack/utils/densenet.py:225\u001b[0m, in \u001b[0;36m_densenet\u001b[0;34m(arch, growth_rate, block_config, num_init_features, pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m pretrained:\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m arch \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdensenet161\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 225\u001b[0m         _load_state_dict(model, model_urls[arch], progress, \u001b[39m'\u001b[39;49m\u001b[39mdensenet161\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    226\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m         _load_state_dict(model, model_urls[arch], progress, \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Year 3/Dis/saliency_patch_attack/utils/densenet.py:208\u001b[0m, in \u001b[0;36m_load_state_dict\u001b[0;34m(model, model_url, progress, flag)\u001b[0m\n\u001b[1;32m    205\u001b[0m pattern \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m    206\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m^(.*denselayer\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.(?:norm|relu|conv))\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.((?:[12])\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.(?:weight|bias|running_mean|running_var))$\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    207\u001b[0m \u001b[39mif\u001b[39;00m flag \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdensenet161\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 208\u001b[0m     state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpretrained_models/densenet161-8d451a50.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    209\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     state_dict \u001b[39m=\u001b[39m load_state_dict_from_url(model_url, progress\u001b[39m=\u001b[39mprogress)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pretrained_models/densenet161-8d451a50.pth'"
     ]
    }
   ],
   "source": [
    "from TranSalNet_Res import TranSalNet\n",
    "model = TranSalNet()\n",
    "model.load_state_dict(torch.load(r'pretrained_models/TranSalNet_Res.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "model = model.to(device) \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-swing",
   "metadata": {},
   "source": [
    "↓↓↓ Get the test image, feed it into the model, and get a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = r'example/COCO_val2014_000000005107.jpg' \n",
    "\n",
    "img = preprocess_img(test_img) # padding and resizing input image into 384x288\n",
    "img = np.array(img)/255.\n",
    "img = np.expand_dims(np.transpose(img,(2,0,1)),axis=0)\n",
    "img = torch.from_numpy(img)\n",
    "img = img.type(torch.cuda.FloatTensor).to(device)\n",
    "pred_saliency = model(img)\n",
    "toPIL = transforms.ToPILImage()\n",
    "pic = toPIL(pred_saliency.squeeze())\n",
    "\n",
    "pred_saliency = postprocess_img(pic, test_img) # restore the image to its original size as the result\n",
    "\n",
    "cv2.imwrite(r'example/result.png', pred_saliency, [int(cv2.IMWRITE_JPEG_QUALITY), 100]) # save the result\n",
    "print('Finished, check the result at: {}'.format(r'example/result.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-mailing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
